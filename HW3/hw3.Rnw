\documentclass[12pt, oldfontcommands]{article}
\usepackage[english]{babel}
%\usepackage[brazilian, brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage[top = 2cm, left = 2cm, right = 2cm, bottom = 2cm]{geometry}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{multicol}
\usepackage[normalem]{ulem}
\usepackage{breqn}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable}
\setlength\parindent{0pt}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}

\title{  
 \normalfont \normalsize 
 \textsc{AMCS 210 - Applied Statistics and Data Analysis} \\
 Hernando Catequista Ombao \\
 Applied Mathematics and Computational Sciences Program \\
 Computer, Electrical and Mathematical Sciences \& Engineering (CEMSE) Division \\
 King Abdullah University of Science and Technology (KAUST) \\[25pt]
 \horrule{.5pt} \\ [.4cm]
 \LARGE HOMEWORK \\
  3
 \horrule{2pt} \\[ .5cm]}
 
\author{Henrique Aparecido Laureano}
\date{\normalsize Fall Semester 2017}

\begin{document}

\maketitle

\vspace{\fill}

\horrule{1pt} \\

\newpage

\vspace{\fill}

\tableofcontents

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\newpage

<<setup, include=FALSE>>=
# <code r> ===================================================================== #
library(knitr)

tema <- knit_theme$get("acid")

knit_theme$set(tema)

opts_chunk$set(size='small'
               , cache=TRUE
               , cache.path='cache/'
               , comment=NA
               , warning=FALSE
               , message=FALSE
               , fig.align='center'
               , dpi=100
               , fig.path='iBagens/'
               , fig.pos='H'
               , background='#ffffff'
               , results='hold'
               , fig.show='hold')
# </code r> ==================================================================== #
@

\section*{Exercise 1} \addcontentsline{toc}{section}{Exercise 1}

\horrule{1pt} \\

\textbf{Exercises on reading tables of distributions.}

\subsection*{(a)} \addcontentsline{toc}{subsection}{(a)}

\textbf{Suppose that a population of scores has a normal distribution with mean
        \(\mu\) = 80 and variance \(\sigma^{2}\) = 9. Use the standard normal
        table to determine the 2.5-th, 5-th, 25-th, 50-th, 75-th, 95-th and
        97.5-th percentile of this distribution of scores. Compare the answers you
        obtained manually by that provided by R (use the help function to
        determine if you need to use pnorm, rnorm, dnorm, etc.)} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the second column (left to right) we have the values from the
          standard normal table; in the third column we have the percentiles
          obtained manually, after the percentiles provided by R and in the end
          the difference between them (we used two decimal places).}
          \vspace{.25cm}
  \begin{tabular}{r|c|r|r|r}
   \toprule
   Percentile & Z & \(Q_{i} = Z \cdot \sigma + \mu\)  & R = \texttt{qnorm(\(Z\), 80, sqrt(9))} & Diff = \(Q_{i}\) - R \\
   \midrule
   2.5-th & -1.96 & \(-1.96 \cdot \sqrt{9} + 80 = \) \Sexpr{-1.96*sqrt(9)+80} & \Sexpr{round(qnorm(.025, 80, sqrt(9)), 2)} & \Sexpr{round(-1.96*sqrt(9)+80 - qnorm(.025, 80, sqrt(9)), 2)} \\
   5-th & -1.64 & \(-1.64 \cdot \sqrt{9} + 80 = \) \Sexpr{-1.64*sqrt(9)+80} & \Sexpr{round(qnorm(.05, 80, sqrt(9)), 2)} & \Sexpr{round(-1.64*sqrt(9)+80 - qnorm(.05, 80, sqrt(9)), 2)} \\
   25-th & -0.67 & \(-0.67 \cdot \sqrt{9} + 80 = \) \Sexpr{-.67*sqrt(9)+80} & \Sexpr{round(qnorm(.25, 80, sqrt(9)), 2)} & \Sexpr{round(-.67*sqrt(9)+80 - qnorm(.25, 80, sqrt(9)), 2)} \\
   50-th & 0.00 & \(0 \cdot \sqrt{9} + 80 = \) \Sexpr{0*sqrt(9)+80} & \Sexpr{qnorm(.5, 80, sqrt(9))} & \Sexpr{0*sqrt(9)+80 - qnorm(.5, 80, sqrt(9))} \\
   75-th & 0.67 & \(0.67 \cdot \sqrt{9} + 80 = \) \Sexpr{.67*sqrt(9)+80} & \Sexpr{round(qnorm(.75, 80, sqrt(9)), 2)} & \Sexpr{round(.67*sqrt(9)+80 - qnorm(.75, 80, sqrt(9)), 2)} \\
   95-th & 1.64 & \(1.64 \cdot \sqrt{9} + 80 = \) \Sexpr{1.64*sqrt(9)+80} & \Sexpr{round(qnorm(.95, 80, sqrt(9)), 2)} & \Sexpr{round(1.64*sqrt(9)+80 - qnorm(.95, 80, sqrt(9)), 2)} \\
   97.5-th & 1.96 & \(1.96 \cdot \sqrt{9} + 80 = \) \Sexpr{1.96*sqrt(9)+80} & \Sexpr{round(qnorm(.975, 80, sqrt(9)), 2)} & \Sexpr{round(1.96*sqrt(9)+80 - qnorm(.975, 80, sqrt(9)), 2)} \\
   \bottomrule
  \end{tabular}
\end{table}

\subsection*{(b)} \addcontentsline{toc}{subsection}{(b)}

\textbf{Determine the 90-th, 95-th and 97.5-th percentile of a
        \(\chi^{2}\)-distribution with} \\
        
\textbf{(i.) degrees of freedom equal to 5.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the second column (left to right) we have the percentiles provided by
          the \(\chi^{2}\) table; in the third column we have the percentiles
          provided by R and in the end the difference between them (we used three
          decimal places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r}
   \toprule
   Percentile & \(\chi^{2}\) (using the table) & \texttt{qchisq(\(\chi^{2}\), 5, lower.tail = FALSE)} & Difference \\
   \midrule
   90-th & \(\chi^{2}_{0.10}\) = 9.236 & \Sexpr{round(qchisq(.1, 5, lower.tail = FALSE), 3)} & \Sexpr{round(9.236 - qchisq(.1, 5, lower.tail = FALSE), 3)} \\
   95-th & \(\chi^{2}_{0.05}\) = 11.070 & \Sexpr{round(qchisq(.05, 5, lower.tail = FALSE), 3)} & \Sexpr{round(11.070 - qchisq(.05, 5, lower.tail = FALSE), 3)} \\
   97.5-th & \(\chi^{2}_{0.025}\) = 12.833 & \Sexpr{round(qchisq(.025, 5, lower.tail = FALSE), 3)} & \Sexpr{round(12.833 - qchisq(.025, 5, lower.tail = FALSE), 3)} \\
   \bottomrule
  \end{tabular}
\end{table}

\textbf{(ii.) with degrees of freedom equal to 10.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the second column we have the percentiles provided by the
          \(\chi^{2}\) table; in the third column we have the percentiles provided
          by R and in the end the difference between them (used three decimal
          places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r}
   \toprule
   Percentile & \(\chi^{2}\) (using the table) & \texttt{qchisq(\(\chi^{2}\), 10, lower.tail = FALSE)} & Difference \\
   \midrule
   90-th & \(\chi^{2}_{0.10}\) = 15.987 & \Sexpr{round(qchisq(.1, 10, lower.tail = FALSE), 3)} & \Sexpr{round(15.987 - qchisq(.1, 10, lower.tail = FALSE), 3)} \\
   95-th & \(\chi^{2}_{0.05}\) = 18.307 & \Sexpr{round(qchisq(.05, 10, lower.tail = FALSE), 3)} & \Sexpr{round(18.307 - qchisq(.05, 10, lower.tail = FALSE), 3)} \\
   97.5-th & \(\chi^{2}_{0.025}\) = 20.483 & \Sexpr{round(qchisq(.025, 10, lower.tail = FALSE), 3)} & \Sexpr{round(20.483 - qchisq(.025, 10, lower.tail = FALSE), 3)} \\
   \bottomrule
  \end{tabular}
\end{table}

\subsection*{(c)} \addcontentsline{toc}{subsection}{(c)}

\textbf{Determine the 2.5-th, 95-th and 97.5-th percentile of a t-distribution
        with} \\

\textbf{(i.) degrees of freedom equal to 5.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the second column we have the percentiles provided by the table,
          using the cumulative probability; in the third column we have the
          percentiles provided by R and in the end the difference between them (we
          used three decimal places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r}
   \toprule
   Percentile & cum. prob (using the table) & R = \texttt{qt({\rm cum. prob}, 5)} & Difference \\
   \midrule
   2.5-th & \(t_{0.025}\) = -2.571 & \Sexpr{round(qt(.025, 5), 3)} & \Sexpr{round(-2.571 - qt(.025, 5), 3)} \\
   95-th & \(t_{0.95}\) = 2.015 & \Sexpr{round(qt(.95, 5), 3)} & \Sexpr{round(2.015 - qt(.95, 5), 3)} \\
   97.5-th & \(t_{0.975}\) = 2.571 & \Sexpr{round(qt(.975, 5), 3)} & \Sexpr{round(2.571 - qt(.975, 5), 3)} \\
   \bottomrule
  \end{tabular}
\end{table}

\textbf{(ii.) with degrees of freedom equal to 1000.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the second column we have the percentiles provided by the table,
          using the cumulative probability; in the third column we have the
          percentiles provided by R and in the end the difference between them (we
          used three decimal places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r}
   \toprule
   Percentile & cum. prob (using the table) & R = \texttt{qt({\rm cum. prob}, 1000)} & Difference \\
   \midrule
   2.5-th & \(t_{0.025}\) = -1.962 & \Sexpr{round(qt(.025, 1e3), 3)} & \Sexpr{round(-1.962 - qt(.025, 1e3), 3)} \\
   95-th & \(t_{0.95}\) = 1.646 & \Sexpr{round(qt(.95, 1e3), 3)} & \Sexpr{round(1.646 - qt(.95, 1e3), 3)} \\
   97.5-th & \(t_{0.975}\) = 1.962 & \Sexpr{round(qt(.975, 1e3), 3)} & \Sexpr{round(1.962 - qt(.975, 1e3), 3)} \\
   \bottomrule
  \end{tabular}
\end{table}

\subsection*{(d)} \addcontentsline{toc}{subsection}{(d)}

\textbf{As the degrees of freedom of a t-distribution increases, show that the
        percentiles becomes closer to that of the standard normal distribution.
        You can choose a few percentiles to demonstrate this (e.g., 2.5-th, 25-th,
        75-th, 97.5-th).} \\

\underline{Solution:} \\

<<echo=FALSE, eval=FALSE>>=
# <code r> ===================================================================== #
da <- data.frame(
  Percentile = rep(c("2.5-th", "25-th", "75-th", "97.5-th"), each = 7)
  , df = rep(c(5, 10, 15, 20, 25, 30, 1000), 4)
  , t = c(-2.571, -2.228, -2.131, -2.086, -2.06, -2.042, -1.962
          , -0.727, -0.7, -0.691, -0.687, -0.684, -0.683, -0.675
          , c(-0.727, -0.7, -0.691, -0.687, -0.684, -0.683, -0.675)*(-1)
          , c(-2.571, -2.228, -2.131, -2.086, -2.06, -2.042, -1.962)*(-1))
  , Z = c(rep(-1.96, 7), rep(-0.674, 7), rep(0.674, 7), rep(1.96, 7)))
da$"|Difference|" <- abs(da[ , 4] - da[ , 3])
library(xtable)
print(xtable(da), include.rownames = FALSE)
# </code r> ==================================================================== #
@

In the Table \ref{table1.d} we see that for every percentile, conform the degrees
of freedom increase, the difference between the \(t\)-distribution and the
Standard Normal distribution is smaller, being zero for very high degrees, like
1000.

\begin{table}[H]
 \centering
 \caption{Percentiles of a t-distribution and standard normal distribution for
          seven different degrees of freedom in four different percentiles. In the
          last column we have the absolute difference between them.}
          \vspace{.25cm}
  \begin{tabular}{r|r|r|r|r}
   \toprule
   Percentile & Degrees of freedom & \(t\)-distribution & Standard Normal distribution & |Difference| \\ 
   \midrule
   2.5-th & 5 & -2.57 & -1.96 & 0.61 \\
   2.5-th & 10 & -2.23 & -1.96 & 0.27 \\
   2.5-th & 15 & -2.13 & -1.96 & 0.17 \\
   2.5-th & 20 & -2.09 & -1.96 & 0.13 \\
   2.5-th & 25 & -2.06 & -1.96 & 0.10 \\
   2.5-th & 30 & -2.04 & -1.96 & 0.08 \\
   2.5-th & 1000 & -1.96 & -1.96 & 0.00 \\
   \midrule
   25-th & 5 & -0.73 & -0.67 & 0.05 \\
   25-th & 10 & -0.70 & -0.67 & 0.03 \\
   25-th & 15 & -0.69 & -0.67 & 0.02 \\
   25-th & 20 & -0.69 & -0.67 & 0.01 \\
   25-th & 25 & -0.68 & -0.67 & 0.01 \\
   25-th & 30 & -0.68 & -0.67 & 0.01 \\
   25-th & 1000 & -0.68 & -0.67 & 0.00 \\
   \midrule
   75-th & 5 & 0.73 & 0.67 & 0.05 \\
   75-th & 10 & 0.70 & 0.67 & 0.03 \\
   75-th & 15 & 0.69 & 0.67 & 0.02 \\
   75-th & 20 & 0.69 & 0.67 & 0.01 \\
   75-th & 25 & 0.68 & 0.67 & 0.01 \\
   75-th & 30 & 0.68 & 0.67 & 0.01 \\
   75-th & 1000 & 0.68 & 0.67 & 0.00 \\
   \midrule
   97.5-th & 5 & 2.57 & 1.96 & 0.61 \\
   97.5-th & 10 & 2.23 & 1.96 & 0.27 \\
   97.5-th & 15 & 2.13 & 1.96 & 0.17 \\
   97.5-th & 20 & 2.09 & 1.96 & 0.13 \\
   97.5-th & 25 & 2.06 & 1.96 & 0.10 \\
   97.5-th & 30 & 2.04 & 1.96 & 0.08 \\
   97.5-th & 1000 & 1.96 & 1.96 & 0.00 \\
   \bottomrule
  \end{tabular}
  \label{table1.d}
\end{table}

We can also see that the difference between this distributions for small
degrees of freedom is smaller for percentiles more closer to the median, the 50-th
percentile, and bigger when the percentile is more closer to the borders (zero and
100-th percentile).

\subsection*{(e)} \addcontentsline{toc}{subsection}{(e)}

\textbf{Determine the 90-th, 95-th and 97.5-th percentile of a F-distribution 
       with} \\

\textbf{(i.) numerator degrees of freedom \(df_{n}\) = 1 and denominator degrees
             of freedom \(df_{d}\) = 10.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the third column we have the percentiles provided by the table; in
          the fourth column we have the percentiles provided by R and in the end
          the difference between them (used two decimal places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r|r}
   \toprule
   Percentile & \(p\) (table) & \(F^{*}\) & \texttt{qf(\(1 - p\), 1, 10)} & Difference \\
   \midrule
   90-th & 0.1 & 3.29 & \Sexpr{round(qf(.9, 1, 10), 2)} & \Sexpr{round(3.29 - qf(.9, 1, 10), 2)} \\
   95-th & 0.05 & 4.96 & \Sexpr{round(qf(.95, 1, 10), 2)} & \Sexpr{round(4.96 - qf(.95, 1, 10), 2)} \\
   97.5-th & 0.025 & 6.94 & \Sexpr{round(qf(.975, 1, 10), 2)} & \Sexpr{round(6.94 - qf(.975, 1, 10), 2)} \\
   \bottomrule
  \end{tabular}
\end{table}

\textbf{(ii.) numerator degrees of freedom \(df_{n}\) = 5 and denominator degrees
              of freedom \(df_{d}\) = 31.} \\

\underline{Solution:}

\begin{table}[H]
 \centering
 \caption{In the third column we have the percentiles provided by the table; in
          the fourth column we have the percentiles provided by R and in the end
          the difference between them (used two decimal places).} \vspace{.25cm}
  \begin{tabular}{r|r|r|r|r}
   \toprule
   Percentile & \(p\) (table) & \(F^{*}\) & \texttt{qf(\(1 - p\), 5, 31)} & Difference \\
   \midrule
   90-th & 0.1 & 2.05 & \Sexpr{round(qf(.9, 5, 31), 2)} & \Sexpr{round(2.05 - qf(.9, 5, 31), 2)} \\
   95-th & 0.05 & 2.53 & \Sexpr{round(qf(.95, 5, 31), 2)} & \Sexpr{round(2.53 - qf(.95, 5, 31), 2)} \\
   97.5-th & 0.025 & 3.03 & \Sexpr{round(qf(.975, 5, 31), 2)} & \Sexpr{round(3.03 - qf(.975, 5, 31), 2)} \\
   \bottomrule
  \end{tabular}
\end{table}

In the table we don't have \(df_{d}\) = 31, so we approximate to 30.

\section*{Exercise 2} \addcontentsline{toc}{section}{Exercise 2}

\horrule{1pt} \\

\textbf{Recall that many of the test statistics are compared to a null
        distribution which is often standard normal (normal with mean equal 0 and
        variance equal to 1). We did not provide any theoretical justification for
        this in class. In this exercise you will need to conduct some simulation
        studies that illustrate this.}

\subsection*{(a)} \addcontentsline{toc}{subsection}{(a)}

\textbf{In testing for the equality of population proportions, the test statistic
        based on random samples of sizes and proportions \(n_{A}\) and
        \(\hat{\pi}_{A}\) from population \(A\) and \(n_{A}\) and
        \(\hat{\pi}_{B}\) from population \(B\) is}

\[ T_{1} = \frac{\hat{\pi}_{A} - \hat{\pi}_{B}}{
                \sqrt{\frac{\hat{\pi}_{A}(1 - \hat{\pi}_{A})}{n_{A}} +
                      \frac{\hat{\pi}_{B}(1 - \hat{\pi}_{B})}{n_{B}}
                     }}.
\]

\textbf{When \(H_{0}: \pi_{A} = \pi_{B}\) is true then the reference distribution
        for \(T_{1}\) is approximately equal to that of a standard normal when
        both sample sizes \(n_{A}\) and \(n_{B}\) are sufficiently large. Conduct
        a simulation study to determine an empirical distribution of \(T_{1}\)
        when the null hypothesis is correct for each pair of sample sizes. Try a
        few pairs, say, (i.) \(n_{A} = n_{B}\) = 30, (ii.) \(n_{A}\) = 25,
        \(n_{B}\) = 40.} \\

\underline{Solution:}

<<2a, fig.width=10, fig.height=4.25, fig.cap="Empirical distribution of \\(T_{1}\\) when the null hypothesis in correct for two different pair of sample sizes. In blue we have the estimated density for this distributions and in red the mean, the 2.5th and 97.5th percentiles.">>=
# <code r> ===================================================================== #
# empty vector of size 1e4 (# of replications) to store the test statistics
ts <- numeric(1e4)
# function to generate the reference distribution
ref.dist <- function(pia = .67, pib = .67, na, nb, title){
  # choosed population proportions: 0.67
  t1 <- function(pia, pib, na, nb){ # test statistic
    (pia - pib) / sqrt( (pia*(1-pia)/na) + (pib*(1-pib)/nb) )
  }
  for (i in 1:length(ts)) { # computing the reference distribution
    hpia <- mean(rbinom(na, 1, pia))
    hpib <- mean(rbinom(nb, 1, pib))
    ts[i] <- t1(pia = hpia, pib = hpib, na = na, nb = nb)
  }
  # histogram
  hist(ts, freq = FALSE, col = "gray60", las = 1, xlab = "", ylab = "", main = NA
       , axes = FALSE) ; axis(side = 1, at = -4:4)
  lines(density(ts), col = "#0080ff", lwd = 3)
  abline(v = c(quantile(ts, .025), mean(ts), quantile(ts, .975))
         , col = 2, lwd = 2)
  mtext(side = 3, text = title, adj = 0, cex = 1.3)
}
par(mfrow = c(1, 2), mar = c(2, 2, 4, 2))
# generating the reference distribution
ref.dist(na = 30, nb = 30, title = bquote(paste(n[A], " = " , n[B], " = 30")))
ref.dist(na = 25, nb = 40, title = bquote(paste(n[A], " = 25, " , n[B], " = 40")))
# </code r> ==================================================================== #
@

We can see in Figure \ref{fig:2a} that in both simulation scenarios we found a 
bell shape behavior, with mean around zero. In red we have the mean, the 2.5th and
97.5th percentiles, and in both cases this percentiles are very close to -2, 2,
which is a sign that the reference distributions are very similar to a Standard
Normal distribution, because we know that in a Standard Normal we have 95\% of the
data between \(\pm\) 1.96 (very close to 2). The mean close to zero is another
sign that the reference distributions are very similar to a Standard Normal
distribution, since in a Standard Normal the mean is zero.

\subsection*{(b)} \addcontentsline{toc}{subsection}{(b)}

\textbf{In testing for the equality of the means of two Gaussian populations with
        common and known variance \(\sigma^{2}\), the test statistic based on the
        sample means \(\bar{X}_{A}\) and \(\bar{X}_{B}\) is}

\[ Z = \frac{\bar{X}_{A} - \bar{X}_{B}}{
            \sqrt{\sigma^{2}\bigg(\frac{1}{n_{A}} + \frac{1}{n_{B}}\bigg)}
                                       }.
\]

\underline{Solution:}

<<2b, fig.width=10, fig.height=4.25, fig.cap="Empirical distribution of \\(Z\\) when the null hypothesis in correct for two different pair of sample sizes. In blue we have the estimated density for this distributions and in red the mean, the 2.5th and 97.5th percentiles.">>=
# <code r> ===================================================================== #
# empty vector of size 1e4 (# of replications) to store the test statistics
zs <- numeric(1e4)
# function to generate the reference distribution
ref.dist <- function(mua = 24, mub = 24, s2 = 32, na, nb, title){
  # choosed population means: 24 ; choosed population variances: 32 
  z <- function(mua, mub, s2, na, nb){ # test statistic
    (mua - mub) / sqrt( s2 * ((1/na) + (1/nb)) )
  }
  for (i in 1:length(zs)) { # computing the reference distribution
    hmua <- mean(rnorm(na, mua, sqrt(s2)))
    hmub <- mean(rnorm(nb, mub, sqrt(s2)))
    zs[i] <- z(mua = hmua, mub = hmub, s2, na = na, nb = nb)
  }
  # histogram
  hist(zs, freq = FALSE, col = "gray60", las = 1, xlab = "", ylab = "", main = NA
       , axes = FALSE) ; axis(side = 1, at = -4:4)
  lines(density(zs), col = "#0080ff", lwd = 3)
  abline(v = c(quantile(zs, .025), mean(ts), quantile(zs, .975))
         , col = 2, lwd = 2)
  mtext(side = 3, text = title, adj = 0, cex = 1.3)
}
par(mfrow = c(1, 2), mar = c(2, 2, 4, 2))
# generating the reference distribution
ref.dist(na = 30, nb = 30, title = bquote(paste(n[A], " = " , n[B], " = 30")))
ref.dist(na = 25, nb = 40, title = bquote(paste(n[A], " = 25, " , n[B], " = 40")))
# </code r> ==================================================================== #
@

We can see in Figure \ref{fig:2b} that in both simulation scenarios we found a 
bell shape behavior, with mean around zero and with 2.5th and 97.5th percentiles
very close to -2, 2. Which is a sign that the reference distributions are very
similar to a Standard Normal distribution.

\subsection*{(c)} \addcontentsline{toc}{subsection}{(c)}

\textbf{When the common variance in the two normal distribution case is not known
        it has to be estimated. Denote the sample variances for group \(A\) to be}

\[ S_{A}^{2} = \frac{1}{n_{A}-1}\sum_{i=1}^{n_{A}}(X_{i}^{A} - \bar{X}_{A})^{2}.
\]

\textbf{The sample variance for group \(B\) is defined in a similar way. Then the
        pooled variance estimate is a weighted average of these two sample
        variances}

\[ S_{P}^{2} = W_{A} S_{A}^{2} + W_{B} S_{B}^{2}  \]

\textbf{where the weights are \(W_{A} = \frac{n_{A}-1}{(n_{A}-1) + (n_{B}-1)}\).
The test statistic for comparing the population means is similar to that of \(Z\)
above but with \(\sigma^{2}\) replaced by its estimator \(S_{P}^{2}\) because it
is not known. Thus,}

\[ T = \frac{\bar{X}_{A} - \bar{X}_{B}}{
            \sqrt{S_{P}^{2}\bigg(\frac{1}{n_{A}} + \frac{1}{n_{B}}\bigg)}
                                       }.
\]

\textbf{The distribution of \(T\) under the null hypothesis is \(t\) with degrees
        of freedom equal to \(d = (n_{A} - 1) + (n_{B} - 1)\). Now conduct
        simulations that demonstrate this theoretical result. You may choose
        whatever settings you like. When you simulate your data, you need to
        specify the variance \(\sigma^{2}\) but when you compute your test
        statistic you need to pretend that you don’t actually know this.} \\

\underline{Solution:}

<<2c, fig.width=10, fig.height=4.25, fig.cap="Empirical distribution of \\(T\\) when the null hypothesis in correct for two different pair of sample sizes. In blue we have the estimated density for this distributions and in red the mean, the 2.5th and 97.5th percentiles.">>=
# <code r> ===================================================================== #
# empty vector of size 1e4 (# of replications) to store the test statistics
ts <- numeric(1e4)
# function to generate the reference distribution
ref.dist <- function(mua = 24, mub = 24, s2 = 32, na, nb, title){
  # choosed population means: 24 ; choosed population variances: 32 
  t <- function(xa, xb, mua, mub, na, nb){ # test statistic
    sa2 <- sum((xa - mua)**2) / (na - 1)
    sb2 <- sum((xb - mub)**2) / (nb - 1)
    wa <- (na - 1) / ((na - 1) + (nb - 1))
    wb <- (nb - 1) / ((na - 1) + (nb - 1))
    sp2 <- wa * sa2 + wb * sb2 # pooled variance estimate
    (mua - mub) / sqrt( sp2 * ((1/na) + (1/nb)) )
  }
  for (i in 1:length(ts)) { # computing the reference distribution
    xa <- rnorm(na, mua, sqrt(s2))
    xb <- rnorm(nb, mub, sqrt(s2))
    hmua <- mean(xa)
    hmub <- mean(xb)
    ts[i] <- t(xa = xa, xb = xb, mua = hmua, mub = hmub, na = na, nb = nb)
  }
  # histogram
  hist(ts, freq = FALSE, col = "gray60", las = 1, xlab = "", ylab = "", main = NA
       , axes = FALSE) ; axis(side = 1, at = -4:4)
  lines(density(ts), col = "#0080ff", lwd = 3)
  abline(v = c(quantile(ts, .025), mean(ts), quantile(ts, .975))
         , col = 2, lwd = 2)
  mtext(side = 3, text = title, adj = 0, cex = 1.3)
}
par(mfrow = c(1, 2), mar = c(2, 2, 4, 2))
# generating the reference distribution
ref.dist(na = 30, nb = 30, title = bquote(paste(n[A], " = " , n[B], " = 30")))
ref.dist(na = 25, nb = 40, title = bquote(paste(n[A], " = 25, " , n[B], " = 40")))
# </code r> ==================================================================== #
@

We can see in Figure \ref{fig:2c} that in both simulation scenarios we found a 
bell shape behavior, with mean around zero and with 2.5th and 97.5th percentiles
very close to -2, 2. Which is a sign that the reference distributions are very
similar to a Standard Normal distribution.

\section*{Exercise 3} \addcontentsline{toc}{section}{Exercise 3}

\horrule{1pt} \\

\textbf{Prior to the 2016 US Presidential elections, a political scientist
        conducted a study to determine if there was any association between
        support for then-candidate DJ Trump and the view on imposing severe
        restriction on immigration. The results of the survey based on a sample of
        \(n\) = 500 respondents are displayed below.}

\begin{center}
 \begin{tabular}{|c|c|c|c|}
  \hline
  & Support restriction & Do not support restriction & Total \\ \hline
  Support DJT & 150 & 50 & 200 \\ \hline
  Do not support DJT & 100 & 200 & 300 \\
  \hline
 \end{tabular}
\end{center}

\subsection*{(a)} \addcontentsline{toc}{subsection}{(a)}

\textbf{What is the proportion of participants who support the immigration
        restriction and at the same time also support DJ Trump?} \\

\underline{Solution:}

\[ {\rm Proportion} =
   \frac{\text{Support restriction} \cap \text{Support DJ Trump}}{n} =
   \frac{150}{500} = \Sexpr{150/500}.
\]

\subsection*{(b)} \addcontentsline{toc}{subsection}{(b)}

\textbf{What is the marginal sample proportion of DJT supporters? What is the
        marginal sample proportion of the restriction on immigration?} \\

\underline{Solution:}

\begin{align*}
 \text{Proportion of DJT supporters} & = \frac{\text{Support DJT}}{n} =
 \frac{200}{500} = \Sexpr{200/500}. \\
 \text{Proportion of the restriction on immigration} & =
 \frac{\text{Support restriction}}{n} = \frac{150 + 100}{500} = \Sexpr{250/500}.
\end{align*}

\subsection*{(c)} \addcontentsline{toc}{subsection}{(c)}

\textbf{Among those who do not support the restriction, what is the proportion of
        DJT supporters? What is the odds of selecting a participant who support
        DJT?} \\

\underline{Solution:}

\begin{align*}
 {\rm Proportion} & =
 \frac{\text{DJT supporters | Do not support the restriction}}{
       \text{Do not support restriction}} = \frac{50}{250} = \Sexpr{50/250}. \\
 {\rm Odds} & = \frac{50/250}{1-50/250} = \Sexpr{(50/250) / (1 - 50/250)}.
\end{align*}

\subsection*{(d)} \addcontentsline{toc}{subsection}{(d)}

\textbf{Conduct a formal test of association between support for DJT and support  
        for restriction on immigration. Note that a formal test should include} \\

\textbf{(i.) the null and alternative hypotheses.} \\

\underline{Solution:} \\

\(H_{0}\): There isn't not a association between support for DJT and whether or
           not someone support restriction on immigration.
\vspace{.4cm} \\
\(H_{a}\): There is a association between support for DJT and whether or not
           someone support restriction on immigration. \\

\textbf{(ii.) the test stastistic.} \\

\underline{Solution:} \\

\(\chi^{2}\) Test.

\begin{align*}
 E_{\text{Support DJT, Support restriction}} & = \frac{200 \cdot 250}{500} =
                                                 \Sexpr{200*250/500} \\
 E_{\text{Support DJT, Don't support restriction}} & = \frac{200 \cdot 250}{500} =
                                                 \Sexpr{200*250/500} \\
 E_{\text{Don't support DJT, Support restriction}} & = \frac{300 \cdot 250}{500} =
                                                 \Sexpr{300*250/500} \\
 E_{\text{Don't support DJT, Don't support restriction}} & =
 \frac{300 \cdot 250}{500} = \Sexpr{300*250/500} \\
\end{align*}

Observed and expected counts are often presented together in a contingency table.
In the table below, expected values are presented in parentheses.

\begin{center}
 \begin{tabular}{|c|c|c|c|}
  \hline
  & Support restriction & Do not support restriction & Total \\ \hline
  Support DJT & 150 (100) & 50 (100) & 200 \\ \hline
  Do not support DJT & 100 (150) & 200 (150) & 300 \\ \hline
  Total & 250 & 250 & 500 \\
  \hline
 \end{tabular}
\end{center}

\(\chi^{2}\) Test Statistic:

\begin{align*}
 \chi^{2} & = \sum \frac{(O - E)^{2}}{E} \\
          & = \frac{(150-100)^{2}}{100} + \frac{(50-100)^{2}}{100} +
              \frac{(100-150)^{2}}{150} + \frac{(200-150)^{2}}{150} \\
          & = \Sexpr{(50**2)/100} + \Sexpr{(50**2)/100} +
              \Sexpr{(50**2)/150} + \Sexpr{(50**2)/150} \\
          & = \Sexpr{(50**2)/100 + (50**2)/100 + (50**2)/150 + (50**2)/150}
\end{align*}

\textbf{(iii.) its null distribution and the rejection region based on the
               probability of Type I error of \(\alpha\) = 0.05.} \\

\underline{Solution:} \\

The \(\chi^{2}\) test statistic is 83.33 with \(df\) = (number of rows - 1)
\(\cdot\) (number of columns - 1) = (2 - 1) \(\cdot\) (2 - 1) = 1. \\

Using the table, we find that for a probability of Type I error of \(\alpha\) =
0.05, with 1 degree of freedom, the rejection region is determined by the value
3.841, which is much smaller than the value of the test statistic. Therefore, we
have significant statistical evidence to don't accept \(H_{0}\). We have
statistical evidence that there is a association between support for DJT and
whether or not someone support restriction on immigration.

\section*{Exercise 4} \addcontentsline{toc}{section}{Exercise 4}

\horrule{1pt} \\

\textbf{Assume that children’s score in the KidScore data set has a
        \(N(\mu_{1}, \sigma^{2})\) distribution if the mother has graduated from
        high school, and \(N(\mu_{2}, \sigma^{2})\) if the mother has not
        graduated from high school. Using the random sample collected in the
        KidScore dataset,}

\subsection*{(a)} \addcontentsline{toc}{subsection}{(a)}

\textbf{Draw the boxplots for each of the two samples. Compare and contrast these
        two boxplots.} \\

\underline{Solution:}

<<4a, fig.width=10, fig.height=5.15, fig.cap="Boxplot for the child's test score at age 3 by mothers that completed, or not, the high school.">>=
# <code r> ===================================================================== #
path <- "~/Dropbox/CLASS-DROPBOX/BOOK-DATA/"
da <- read.table(paste0(path, "KidScore.txt"), header = TRUE, sep = ",")
da$momHs <- with(da, factor(momHs, labels = c("No", "Yes")))

boxplot(kidScore ~ momHs, da, las = 1, xlab = "Mother Completed High School?"
        , main = "Child's Test Score at Age 3")
# </code r> ==================================================================== #
@

We can see in the Figure \ref{fig:4a} that childrens of mothers that completed the
high school present biggest medians and smallest variance, than the childrens of
mothers that didn't completed the high school.

\subsection*{(b)} \addcontentsline{toc}{subsection}{(b)}

\textbf{Compute the sample means and variances for each of the two groups.} \\

\underline{Solution:}

<<>>=
# <code r> ===================================================================== #
library(plyr)
ddply(da, .(momHs), summarise, Mean = mean(kidScore), Variance = var(kidScore))
# </code r> ==================================================================== #
@

\subsection*{(c)} \addcontentsline{toc}{subsection}{(c)}

\textbf{Compute the pooled variance estimate \(S_{P}^{2}\).} \\

\underline{Solution:}

\[ \text{Pooled variance:} \qquad
   S_{P}^{2} = W_{\text{momHs: No}} S_{\text{momHs: No}}^{2} +
               W_{\text{momHs: Yes}} S_{\text{momHs: Yes}}^{2}.
\]
\begin{align*}
 S_{\text{momHs: No}}^{2} & = \frac{1}{n_{\text{momHs: No}}-1}
 \sum_{i=1}^{n_{\text{momHs: No}}}
 \Big(X_{i}^{\text{momHs: No}}-\bar{X}_{\text{momHs: No}}\Big)^{2} =
 \Sexpr{sum(with(subset(da, momHs == "No")
        , kidScore-mean(kidScore))**2)/(table(da$momHs)[[1]]-1)}. \\
 W_{\text{momHs: No}} & = \frac{n_{\text{momHs: No}}-1}{
                           (n_{\text{momHs: No}}-1) + (n_{\text{momHs: Yes}}-1)} =
 \frac{92}{92 + 340} = \Sexpr{92/(92+340)}. \\
 S_{\text{momHs: Yes}}^{2} & = \frac{1}{n_{\text{momHs: Yes}}-1}
 \sum_{i=1}^{n_{\text{momHs: Yes}}}
 \Big(X_{i}^{\text{momHs: Yes}}-\bar{X}_{\text{momHs: Yes}}\Big)^{2} =
 \Sexpr{sum(with(subset(da, momHs == "Yes")
        , kidScore-mean(kidScore))**2)/(table(da$momHs)[[2]]-1)}. \\
 W_{\text{momHs: Yes}} & = \frac{n_{\text{momHs: Yes}}-1}{
                           (n_{\text{momHs: Yes}}-1) + (n_{\text{momHs: No}}-1)} =
 \frac{340}{340 + 92} = \Sexpr{340/(340+92)}.
\end{align*}
\[ \boxed{
   S_{P}^{2} =
   \Sexpr{(sum(with(subset(da, momHs == "No")
          , kidScore-mean(kidScore))**2)/(table(da$momHs)[[1]]-1))*(92/(92+340))}
   + \Sexpr{(sum(with(subset(da, momHs == "Yes")
            , kidScore-mean(kidScore))**2)/(table(da$momHs)[[2]]-1))*
            (340/(340+92))} =
   \Sexpr{(sum(with(subset(da, momHs == "No")
          , kidScore-mean(kidScore))**2)/(table(da$momHs)[[1]]-1))*(92/(92+340)) +
          (sum(with(subset(da, momHs == "Yes")
          , kidScore-mean(kidScore))**2)/(table(da$momHs)[[2]]-1))*(340/(340+92))}
        .}
\]

\subsection*{(d)} \addcontentsline{toc}{subsection}{(d)}

\textbf{Conduct a formal test for comparing the means \(\mu_{1}\) and \(\mu_{2}\).
        Again - you already should know the complete information that is needed to
        conduct this test.} \\

\underline{Solution:} \\

We will test the \textbf{hypothese} that the \textbf{difference} between the mean
of the child's test score at age 3 of mothers that completed the high school 
\textbf{are not statiscally significant} to the mean of the child's test score at
age 3 of mothers that didn't completed the high school.

\[ {\rm Hypotheses:} \qquad
   H_{0}: \mu_{\text{momHs: No}} = \mu_{\text{momHs: Yes}}, \qquad
   H_{a} = \mu_{\text{momHs: No}} \neq \mu_{\text{momHs: Yes}}.
\]

<<echo=FALSE>>=
# <code r> ===================================================================== #
nhs <- subset(da, momHs == "No")
hs <- subset(da, momHs == "Yes")
# </code r> ==================================================================== #
@

\begin{align*}
 \text{Test Statistic:} \quad
 T & = \frac{\bar{X}_{\text{momHs: No}}-\bar{X}_{\text{momHs: Yes}}}{
        \sqrt{S_{P}^{2}\bigg(\frac{1}{n_{\text{momHs: No}}} + 
                             \frac{1}{n_{\text{momHs: Yes}}}\bigg)}} \\
   & = \frac{\Sexpr{mean(nhs$kidScore)}-\Sexpr{mean(hs$kidScore)}}{
             \sqrt{394.1231264 \cdot
                   \Big(\frac{1}{\Sexpr{nrow(nhs)}}+
                        \frac{1}{\Sexpr{nrow(hs)}}\Big)
                  }} \\
   & = \Sexpr{(mean(nhs$kidScore)-mean(hs$kidScore))/
              sqrt(394.1231264*((1/nrow(nhs))+(1/nrow(hs))))}
\end{align*}

Reference distribution: \(t\)-Student with
\((n_{\text{momHs: No}}-1)+(n_{\text{momHs: Yes}}-1) = \Sexpr{92+340}\) degrees of
freedom. We will also consider an \(\alpha = 0.05\). \\

For this probability of Type I error and this amount of degrees of freedom, the 
critical value is

<<>>=
# <code r> ===================================================================== #
qt(.975, 432)
# </code r> ==================================================================== #
@

\(t_{0.025, 432} = -1.965471\), \(t_{0.975, 432} = 1.965471\). Extremely closer
to a Standard Normal. \\

\fbox{As our Test Statistic is less than the critical value, we don't accept the
      \(H_{0}\).} \\

\fbox{\begin{minipage}{\linewidth}
       We have statistical evidence to don't accept \(H_{0}\), in other words, we
       have evidence that exist difference statiscally significant between the
       mean of the child's test score at age 3 of mothers that completed the high
       school and the mean of the child's test score at age 3 of mothers that
       didn't completed the high school.
      \end{minipage}} \\

\hfill \(\blacksquare\)

\horrule{.5pt}

\vspace{\fill}

\horrule{1pt} \\

\end{document}